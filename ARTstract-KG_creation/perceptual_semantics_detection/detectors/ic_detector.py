"""
Image Captioning with Vision Transformer (ViT) and GPT-2

This script utilizes the VisionEncoderDecoderModel and ViTImageProcessor from the Hugging Face Transformers library to perform image captioning using the GPT-2 language model. The script loads a pre-trained ViT-GPT-2 model along with its feature extractor and tokenizer. It then generates descriptive captions for input images within a specified folder and saves the results in JSON format.

The VisionEncoderDecoderModel is a transformer-based model that combines ViT (Vision Transformer) for image processing and GPT-2 for language generation, making it suitable for image captioning tasks. The ViTImageProcessor processes the input images to extract relevant features for the model, while the AutoTokenizer handles tokenization of the generated captions.

Parameters:

max_length (int): The maximum length of the generated captions. You can set this parameter to control the length of the captions generated by the model.
num_beams (int): The number of beams used during caption generation with beam search. Increasing this value can improve the diversity of generated captions, but it also increases computation time.

Note:

The script uses the nlpconnect/vit-gpt2-image-captioning pre-trained model for image captioning.
Ensure that you have the required Hugging Face Transformers library installed to execute the script successfully.
The paths and naming conventions for the images and the JSON output should be adjusted as needed for your specific dataset.
The ViTImageProcessor is used to preprocess the images before feeding them to the ViT-GPT-2 model for caption generation.
The max_length and num_beams parameters can be adjusted to control the length and diversity of the generated captions, respectively.
The script automatically generates captions for images in the specified folders and provides an example of usage for multiple concept folders.
The script assumes that each image in the folder has a unique identifier (e.g., image_id) obtained by removing the file extension from the image filename. The descriptions are then stored in the JSON file with the corresponding image_id as the key.
"""
import os
import json
import torch
from PIL import Image
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration


def process_folder(annotation_situation, folder_path, output_file):
    """
    Processes a folder containing multiple images and generates image descriptions for each image.
    The results are stored in a JSON file with the specified `output_file` name.

    Parameters:
        folder_path (str): Path to the folder containing the images.
        output_file (str): Path to the JSON file where the results will be saved.
    """

    annotator = annotation_situation["annotator"]
    def get_image_description(image_path, annotator):
        """
        Processes a folder containing multiple images and generates image descriptions for each image.
        The results are stored in a JSON file with the specified `output_file` name.

        Parameters:
            folder_path (str): Path to the folder containing the images.
            output_file (str): Path to the JSON file where the results will be saved.
        """
        image = Image.open(image_path)
        if image.mode != "RGB":
            image = image.convert(mode="RGB")

        if annotator == "nlpconnect/vit-gpt2-image-captioning":
            model = VisionEncoderDecoderModel.from_pretrained(annotator)
            feature_extractor = ViTImageProcessor.from_pretrained(annotator)
            tokenizer = AutoTokenizer.from_pretrained(annotator)

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model.to(device)

            max_length = 30
            num_beams = 4
            gen_kwargs = {"max_length": max_length, "num_beams": num_beams}
            pixel_values = feature_extractor(images=[image], return_tensors="pt").pixel_values
            pixel_values = pixel_values.to(device)

            output_ids = model.generate(pixel_values, **gen_kwargs)

            preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
            preds = [pred.strip() for pred in preds]
            pred = preds[0] if preds else None

        elif annotator == "Salesforce/blip-image-captioning-large":
            image = Image.open(image_path)
            if image.mode != "RGB":
                image = image.convert(mode="RGB")
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            processor = BlipProcessor.from_pretrained(annotator)
            model = BlipForConditionalGeneration.from_pretrained(annotator).to(device)

            text = ""
            inputs = processor(image, text, return_tensors="pt").to(device)

            out = model.generate(**inputs)
            pred = processor.decode(out[0], skip_special_tokens=True)
        return pred

    folder_ics = {}
    annotation_situation_name = str(
        annotation_situation["annotated_dataset"] + "_" + annotation_situation["annotation_type"] + "_" +
        annotation_situation["annotation_time"])
    annotation_type = annotation_situation["annotation_type"]

    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg") or filename.endswith(".png"):
            image_path = os.path.join(folder_path, filename)
            image_id = os.path.splitext(filename)[0]
            ic = get_image_description(image_path, annotator)

            folder_ics[image_id] = {}
            folder_ics[image_id][annotation_type] = {}
            folder_ics[image_id][annotation_type][annotation_situation_name] = {
                "image_description": ic,
            }

    with open(output_file, "w") as file:
        json.dump(folder_ics, file, indent=2)

    print("Processing completed. JSON file generated:", output_file)
    return



# annotation_situation = {
#     "annotation_type" : "ic",
#     # "annotator" : "nlpconnect/vit-gpt2-image-captioning",
#     "annotator" : "Salesforce/blip-image-captioning-large",
#     "annotation_place" : "Italy",
#     "annotation_time" : "2023_06_28",
#     "detection_threshold": "top one",
#     "annotated_dataset": "ARTstract"
# }
#
# folder_path = '../../__prova/test'
#
# output_file = f'../{annotation_situation["annotation_type"]}_output.json'
#
# process_folder(annotation_situation, folder_path, output_file)
#

